{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "b-In0GDZqOGo",
        "N_cD_EG71V9_",
        "gCjY4locoejt"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "os6ws_aF58Go"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Emf_x3ZsQAw1"
      },
      "outputs": [],
      "source": [
        "# Install python packages\n",
        "!pip install datasets transformers[torch] accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload training file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "9yVJn7dKQO_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the fine-tuning data - *** edit filename below if necessary ***\n",
        "with open(\"filename.json\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Reduce dataset size to n samples if needed:\n",
        "# n = 100\n",
        "# data = data[:n]\n",
        "\n",
        "# Create a Dataset\n",
        "dataset = Dataset.from_dict(formatted_data)\n"
      ],
      "metadata": {
        "id": "L5TMEwsLS9l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TinyLlama 1.1B Chat"
      ],
      "metadata": {
        "id": "b-In0GDZqOGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TinyLlama 1.1B Chat\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples[\"input_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples[\"target_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
        "\n",
        "# Set up training arguments with reduced batch size and max sequence length\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,  # Further reduced batch size\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,  # Accumulate gradients over 8 steps\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Check if GPU is available and move the model to GPU\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "JjC_Z7k4SLoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Microsoft Phi-1.5"
      ],
      "metadata": {
        "id": "N_cD_EG71V9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Microsoft Phi-1.5\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"microsoft/phi-1_5\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Add a padding token if not already present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Ensure use_cache is set to False in the model configuration\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples[\"input_text\"], max_length=32, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples[\"target_text\"], max_length=32, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Assuming 'dataset' is already loaded as a Dataset object\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,  # Increased to accumulate more gradients\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Check if GPU is available and move the model to GPU\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./finetuned_model\")\n",
        "tokenizer.save_pretrained(\"./finetuned_model\")\n",
        "\n",
        "# Zip the model directory\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = './finetuned_model.zip'\n",
        "unzip_dir = './finetuned_model'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "    for root, dirs, files in os.walk(unzip_dir):\n",
        "        for file in files:\n",
        "            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), unzip_dir))\n",
        "\n",
        "# Optional: Download the model zip file to local machine\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "1OG4Gy1FwwZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama-2-7b\n",
        "(will need to auth via Hugging Face and accept Meta's terms)"
      ],
      "metadata": {
        "id": "gCjY4locoejt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install hugging face hub\n",
        "!pip install huggingface_hub\n",
        "\n",
        "# Authenticate with Hugging Face - enter auth token in cell below\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "id": "p3rQIfH2eOeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU cache\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load the tokenizer and model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat\"  # Replace with the actual model name on Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Add a padding token if not already present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Ensure use_cache is set to False in the model configuration\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples[\"input_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples[\"target_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Assuming 'dataset' is already loaded as a Dataset object\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,  # Increased to accumulate more gradients\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Check if GPU is available and move the model to GPU\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"./finetuned_model\")\n",
        "tokenizer.save_pretrained(\"./finetuned_model\")\n",
        "\n",
        "# Zip the model directory\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = './finetuned_model.zip'\n",
        "unzip_dir = './finetuned_model'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "    for root, dirs, files in os.walk(unzip_dir):\n",
        "        for file in files:\n",
        "            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), unzip_dir))\n",
        "\n",
        "# Optional: Download the model zip file to local machine\n",
        "from google.colab import files\n",
        "files.download(zip_path)\n"
      ],
      "metadata": {
        "id": "IEYvYg5YwUrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load the tokenizer and model\n",
        "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Add a padding token if not already present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Ensure use_cache is set to False in the model configuration\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples[\"input_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples[\"target_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n"
      ],
      "metadata": {
        "id": "a88Z9ZtatydD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=1,  # Reduced batch size\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,  # Increased to accumulate more gradients\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        ")\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Check if GPU is available and move the model to GPU\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n"
      ],
      "metadata": {
        "id": "MMhu8NBGu7yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "DSNdNWBCvDch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deepseek Coder\n"
      ],
      "metadata": {
        "id": "k5ss9apCOah1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the tokenizer and model\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "import torch\n",
        "model_name = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples[\"input_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples[\"target_text\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Assuming 'dataset' is defined somewhere in your code\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"input_text\", \"target_text\"])\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=1e-5,  # Reduced learning rate\n",
        "    per_device_train_batch_size=2,  # Adjust according to the model capabilities\n",
        "    per_device_eval_batch_size=2,  # Adjust if necessary\n",
        "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
        "    num_train_epochs=5,  # Increase number of epochs\n",
        "    weight_decay=0.01,\n",
        "    fp16=False,  # Adjust if supported\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,  # More frequent logging\n",
        "    save_total_limit=2,\n",
        "    save_steps=250,\n",
        ")\n",
        "\n",
        "# Check if GPU is available and move the model to GPU\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    eval_dataset=tokenized_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "XZysuGerOYqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model\n"
      ],
      "metadata": {
        "id": "VGkuFsf5QM3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save model files\n",
        "model.save_pretrained(\"./finetuned_model\")"
      ],
      "metadata": {
        "id": "1rOJBkhCQFqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save tokenizer files\n",
        "tokenizer.save_pretrained(\"./finetuned_model\")"
      ],
      "metadata": {
        "id": "vWsTlFNGD3lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download fine tuned model as zip file\n",
        "!zip -r finetuned_model.zip ./finetuned_model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"finetuned_model.zip\")\n"
      ],
      "metadata": {
        "id": "KezgFtvDD-bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU cache if maxed out\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "fciGYw-4IZDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the drive for model persistence\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bihLrv0MECcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download to Google Drive\n",
        "!cp finetuned_model.zip /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "BByrw0cOIs8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **-------------------------------**\n",
        "# Use the model from the zip file"
      ],
      "metadata": {
        "id": "8Q7hG5C0N9AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount the drive if you haven't yet in previous lines\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "vnZBQU0_JTiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the file\n",
        "import zipfile\n",
        "\n",
        "# Change the path to where your zip file is located\n",
        "zip_path = '/content/drive/MyDrive/finetuned_model.zip'\n",
        "unzip_dir = '/content/finetuned_model'\n",
        "\n",
        "# Unzip the model\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_dir)\n"
      ],
      "metadata": {
        "id": "sPj4cWF4OI9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# Load the tokenizer and model from the saved directory - verify\n",
        "model_path = \"./finetuned_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# Example input text\n",
        "input_text = \"CUSTOM_DB_42_QUERY Find all claims in master_claim_schema.claim_info with suspicious descriptions that include the word 'arson'.\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Control output -- minimal config\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=128,\n",
        "    repetition_penalty=1.4\n",
        ")\n",
        "\n",
        "# Control output -- more default version of config - tested well with DeepSeek Coder\n",
        "# outputs = model.generate(\n",
        "#     **inputs,\n",
        "#     max_length=128,  # Adjust max_length to a suitable value for your SQL queries\n",
        "#     repetition_penalty=1.0,  # Reset to default\n",
        "#     temperature=1.0,  # Reset to default, controls the randomness of predictions\n",
        "#     top_k=50,  # Default value, controls sampling diversity\n",
        "#     top_p=1.0,  # Default value, controls nucleus sampling\n",
        "#     num_beams=1  # Default value, no beam search\n",
        "# )\n",
        "\n",
        "# Custom outputs\n",
        "# outputs = model.generate(\n",
        "#     **inputs,\n",
        "#     max_length=128,  # Adjust max_length to a suitable value for your SQL queries\n",
        "#     repetition_penalty=1.3,\n",
        "#     temperature=1.0,  # Reset to default, controls the randomness of predictions\n",
        "#     top_k=50,  # Default value, controls sampling diversity\n",
        "#     top_p=1.0,  # Default value, controls nucleus sampling\n",
        "#     num_beams=1  # Default value, no beam search\n",
        "# )\n",
        "\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "0R2vp0LZOTbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mkmd9QRfNQ1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}